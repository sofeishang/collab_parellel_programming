import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as ss
import math
import json
import random
import itertools
import copy
import warnings
import itertools

pd.set_option('display.max_columns', 300)
pd.set_option('display.max_rows', None)
warnings.filterwarnings('ignore')

# Generate Schedule
def paramOrder (block):
    param_v = []
    param_n = []
    for b in block:
        if b == "HighVolatility_HighNoise":
            param_v.append(high_volatility_kappa)
            param_n.append(high_noise_kappa)
        if b == "HighVolatility_LowNoise":
            param_v.append(high_volatility_kappa)
            param_n.append(low_noise_kappa)
        if b == "LowVolatility_LowNoise":
            param_v.append(low_volatility_kappa)
            param_n.append(low_noise_kappa)
        if b == "LowVolatility_HighNoise":
            param_v.append(low_volatility_kappa)
            param_n.append(high_noise_kappa)
    return param_v, param_n

def generateDrifts (high_volatility_kappa, low_volatility_kappa, n_trials, n_blocks):
    """
    param_v = [high_volatility_kappa * 50, high_volatility_kappa * 50, 
                low_volatility_kappa * 50,  low_volatility_kappa * 50]
    Drift are generated through a von misese distribution;
    The first drift of every block will be zero;
    """
    step = []
    volatility_block = [high_volatility_kappa, low_volatility_kappa]
    for v in volatility_block:
        n = 0
        while n < n_trials:
            if n == 0:
                step.append(0)
                n = n + 1
            else:
                step.append(np.random.vonmises(0, v))
                n = n + 1
    step = step[:n_trials] * 2 + step[n_trials:] * 2
    return step

def generateMean(step):
    """
    The mean of first trial of each block is randomly generated. 
    The mean of subsequent trials of each block is the mean of the previous trial + drift
    """
    mean = []
    volatility_block = [step[:n_trials], step[-n_trials:]] # high, low
    m = 0 # used to index mean 
    for v in volatility_block:
        n = 0 # used to index drift
        while n < n_trials:
            if n == 0: 
                initial_value = np.random.uniform(low = -math.pi, high = math.pi)
                mean.append(initial_value)
                n = n + 1
                m = m + 1
            else:
                mean.append(mean[m-1] + v[n])
                m = m + 1
                n = n + 1
    mean = mean[:n_trials] * 2 + mean[n_trials:] * 2
    return mean

def generateAngle (mean, param_n):
    """
    Angle is generated by a von mises distribution with pre-determined kappa value and mean computed above
    """
    n = 0
    angle = []
    while n < n_blocks*n_trials:
        a = np.random.vonmises(mean[n], param_n[n])
        angle.append(a)
        n = n + 1
    return angle

def normalize(angle):
    res = angle
    while res > math.pi:
        res -= 2.0*math.pi
    while res < -math.pi:
        res += 2.0*math.pi
    return res


def generateData():
    param_v = paramOrder(block)[0]
    param_n = paramOrder(block)[1]
    drifts = generateDrifts(high_volatility_kappa, low_volatility_kappa, n_trials, n_blocks)
    mean = generateMean(drifts)
    angle = generateAngle(mean, param_n)
    trial = np.arange(1, n_trials+1).tolist()*4

    df = pd.DataFrame(
    {'trial': trial,
    'block': block,
     'kappa_for_drift': param_v,
     'drifts': drifts,
     "mean": mean,
     "kappa_for_noise": param_n,
     "angle": angle
    })
    df["volatility"] = np.where(df.block.str.contains("HighVolatility"), "High", "Low")
    df["noise"] = np.where(df.block.str.contains("HighNoise"), "High", "Low")
    df["angle_-pi_pi"] = df["angle"].apply(normalize)
    df["angle_normalized"] = np.where(df["angle_-pi_pi"]<0, 2*math.pi+df["angle_-pi_pi"], df["angle_-pi_pi"])
    df["mean_-pi_pi"] = df["mean"].apply(normalize)
    df["mean_normalized"] = np.where(df["mean_-pi_pi"]<0, 2*math.pi+df["mean_-pi_pi"], df["mean_-pi_pi"])
    return df

# Simulation
def signed_diff (a, b):
    raw_diff = a - b
    absolute_diff = abs(raw_diff)
    if absolute_diff > math.pi:
        converted_diff = 2*math.pi - absolute_diff
        to_ret = np.sign(raw_diff) * converted_diff
    else:
        to_ret = raw_diff
    return to_ret

def next_prediction (lr, current_belief, outcome):
    learnt = lr * signed_diff(outcome, current_belief)
    return current_belief + learnt


def check_points (target, prediction, bucket_size):
    left = -np.deg2rad(bucket_size)/2
    right = np.deg2rad(bucket_size)/2
    range_lower = prediction + left
    range_higher = prediction + right
    if ((target < range_higher) & (target > range_lower)):
        return 1
    else:
        return 0

def run_mini_sim_fixed_lr_fixed_start (block_name, lr, bucket_size, data):
    index = 0
    outcome = data[data.block == block_name]["angle_normalized"].tolist()[:n_trials]
    points = []
    prediction = []
    schedule = []
    tracked_lr = []
    while index < n_trials:
        o = outcome[index]
        if index == 0:
            initial_belief = o
            prediction.append(initial_belief)
            p = check_points(o, initial_belief, bucket_size)
            points.append(p)
            belief = next_prediction (lr, initial_belief, o)
            schedule.append(o)
            tracked_lr.append(lr)
            index = index + 1
        else:
            p = check_points(o, belief, bucket_size)
            prediction.append(belief)
            points.append(p)
            belief = next_prediction (lr, belief, o)
            schedule.append(o)
            tracked_lr.append(lr)
            index = index + 1
    dataframe = pd.DataFrame(
        {'lr': tracked_lr,
         'trial': np.arange(1, n_trials+1),
         'block': [block_name]*n_trials,
         'coin_angle': schedule,
         'predicted_position': prediction,
         'outcome': points
        })
    return points, dataframe

lr = np.arange(0, 1.01, 0.01)

def get_data (data, lr):
    simulated_df = []
    hv_hn_points = []
    hv_hn_prediction = []
    hv_ln_points = []
    hv_ln_prediction = []
    lv_hn_points = []
    lv_hn_prediction = []
    lv_ln_points = []
    lv_ln_prediction = []
    for l in lr:
        p = run_mini_sim_fixed_lr_fixed_start ("HighVolatility_HighNoise", l, 60, data)[0]
        hv_hn_points.append(p)
        simulated_df.append(run_mini_sim_fixed_lr_fixed_start ("HighVolatility_HighNoise", l, 60, data)[1])
    
        p = run_mini_sim_fixed_lr_fixed_start ("LowVolatility_HighNoise", l, 60, data)[0]
        lv_hn_points.append(p)
        simulated_df.append(run_mini_sim_fixed_lr_fixed_start ("LowVolatility_HighNoise", l, 60, data)[1])
    
        p = run_mini_sim_fixed_lr_fixed_start ("HighVolatility_LowNoise", l, 30, data)[0]
        hv_ln_points.append(p)
        simulated_df.append(run_mini_sim_fixed_lr_fixed_start ("HighVolatility_LowNoise", l, 30, data)[1])
    
        p = run_mini_sim_fixed_lr_fixed_start ("LowVolatility_LowNoise", l, 30, data)[0]
        lv_ln_points.append(p)
        simulated_df.append(run_mini_sim_fixed_lr_fixed_start ("LowVolatility_LowNoise", l, 30, data)[1])
        
    simulated_df = pd.concat(simulated_df)
    
    p1 = []
    for p in hv_hn_points:
        p1.append(np.sum(p))
    p2 = []
    for p in lv_hn_points:
        p2.append(np.sum(p))
    p3 = []
    for p in hv_ln_points:
        p3.append(np.sum(p))
    p4 = []
    for p in lv_ln_points:
        p4.append(np.sum(p))
    
    final_points = pd.DataFrame(
    {'lr': lr,
     'HighVolatility_HighNoise': p1,
     'LowVolatility_HighNoise': p2,
     'HighVolatility_LowNoise': p3,
     'LowVolatility_LowNoise': p4
    })
    final_points_long = pd.melt(final_points, id_vars='lr', value_vars=['HighVolatility_HighNoise', 'LowVolatility_HighNoise', 
                                                'HighVolatility_LowNoise', 'LowVolatility_LowNoise'])
    final_points_long.rename(columns = {'variable':'block', 
                                    'value':'points_earned'}, inplace = True)

    
    return simulated_df, final_points_long

# Metrics
def marginals (points):
    denominator = np.sum(points)
    return [p/denominator for p in points]

def weighted_avg_and_std(values, weights):
    """
    Return the weighted average and standard deviation.

    values, weights -- Numpy ndarrays with the same shape.
    """
    average = np.average(values, weights=weights)
    # Fast and numerically precise:
    variance = np.average((values-average)**2, weights=weights)
    return (average, math.sqrt(variance))

def form_metrics (schedule, simulation_points):
    
    block_list = []
    weighted_average = []
    weighted_dispersion = []
    volatility_effect = []
    noise_effect = []
    dispersion_blocks = []

    for b in block_name:
        # block_name = ["HighVolatility_HighNoise", "HighVolatility_LowNoise", 
        #  "LowVolatility_LowNoise", "LowVolatility_HighNoise"]
        block_list.append(b)
        d = simulation_points[simulation_points.block == b]
        weights = marginals(d.points_earned)
        weighted_average.append(weighted_avg_and_std(d.lr, weights)[0])
        weighted_dispersion.append(weighted_avg_and_std(d.lr, weights)[1])

    volatility_effect.append((weighted_average[0] + weighted_average[1]) - (weighted_average[2] + weighted_average[3]))
    noise_effect.append((weighted_average[1] + weighted_average[2]) - (weighted_average[0] + weighted_average[3]))
    dispersion_blocks.append(np.sum(weighted_dispersion))
    
    schedule.groupby("volatility").drifts.mean()["High"]
    
    raw_metrics = pd.DataFrame({
        "block": block_list,
        "weighted_average": weighted_average,
        "weighted_dispersion": weighted_dispersion
    })
    
    metrics = pd.DataFrame({
        "volatility_effect": volatility_effect,
        "noise_effect": noise_effect,
        "dispersion_blocks": dispersion_blocks})
    
    return raw_metrics, metrics

# Integrate
def integrate (index_n):
    
    # call functions
    schedule = generateData()
    schedule["index"] = [index_n]*len(schedule)
    
    simulation = get_data(schedule, lr)
    simulation_predicted_angle = simulation[0]
    simulation_predicted_angle["index"] = [index_n]*len(simulation_predicted_angle)
    simulation_points_earned = simulation[1]
    simulation_points_earned["index"] = [index_n]*len(simulation_points_earned)
    
    raw_metrics = form_metrics(schedule, simulation_points_earned)[0]
    raw_metrics["index"] = [index_n]*len(raw_metrics)
    
    metrics = form_metrics(schedule, simulation_points_earned)[1]
    metrics["index"] = [index_n]*len(metrics)
    
    return [schedule, simulation_predicted_angle, simulation_points_earned, raw_metrics, metrics]
    

# Run
# high_volatility_kappa = 8 (7) (13)
# low_volatility_kappa = 80 (78) (82)
# high_noise_kappa = 3 (25) (19)
# low_noise_kappa = 35 (46) (28)

# volatility_values = [[high volatility, low volatility]]
high_v_kappa = np.arange(2, 10, 1)
# 27 # 17 # step size = 2 # step size changed back to 1
low_v_kappa = np.arange(85, 92, 1)
# 70 # 80 # 85 step size to 1
volatility_values = list(itertools.product(high_v_kappa, low_v_kappa))
volatility_values = [v for v in volatility_values if v[0] < v[1]]

# noise_values = [[high noise, low noise]]
high_n_kappa = np.arange(5, 21, 1)
# 27 # 10
low_n_kappa = np.arange(20, 41, 1)
noise_values = list(itertools.product(high_n_kappa, low_n_kappa))
noise_values = [n for n in noise_values if n[0] < n[1]]

index = []
schedule_all = []
simulation_predicted_angle_all = []
simulation_points_earned_all = []
raw_metrics_all = []
metrics_all = []
index_n = 0

for v in volatility_values:
    print (v)
    for n in noise_values:
        print (n)
        # define values
        n_blocks = 4
        n_trials = 50
        high_volatility_kappa = v[0]
        low_volatility_kappa = v[1]
        high_noise_kappa = n[0]
        low_noise_kappa = n[1]
        block_name = ["HighVolatility_HighNoise", "HighVolatility_LowNoise", 
              "LowVolatility_LowNoise", "LowVolatility_HighNoise"]
        block = np.repeat(block_name, n_trials)
        
        # call functions
        data = integrate(index_n)
        
        # append
        schedule_all.append(data[0])
        simulation_predicted_angle_all.append(data[1])
        simulation_points_earned_all.append(data[2])
        raw_metrics_all.append(data[3])
        metrics_all.append(data[4])
        
        # increment
        index_n = index_n + 1

export_path = "/Users/sophies/Desktop/Year1/Project1/Simulation for new parameters/Data_June/June_15th/"

schedule_df = pd.concat(schedule_all)
schedule_df.to_csv(export_path + "schedule_df.csv")

simulation_points_earned_df = pd.concat(simulation_points_earned_all)
simulation_points_earned_df.to_csv(export_path + "simulation_points_earned_df.csv")

raw_metrics_df = pd.concat(raw_metrics_all)
raw_metrics_df.to_csv(export_path + "raw_metrics_df.csv")

metrics_df = pd.concat(metrics_all)
metrics_df.to_csv(export_path + "metrics_df.csv")